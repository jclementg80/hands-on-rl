{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c2ff31f",
   "metadata": {},
   "source": [
    "# 00 Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04a5c882",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰Before you solve a Reinforcement Learning problem you need to define what are\n",
    "- the actions\n",
    "- the states of the world\n",
    "- the rewards\n",
    "\n",
    "#### ðŸ‘‰We are using the `Taxi-v3` environment from OpenAI's gym: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "#### ðŸ‘‰`Taxi-v3` is an easy environment because the action space is small, and the state space is large but finite.\n",
    "\n",
    "#### ðŸ‘‰Environments with a finite number of actions and states are called tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3629346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76e9a06d",
   "metadata": {},
   "source": [
    "## Load the environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebfba291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\").env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fcfc13a",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cfdb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f53a38e",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e809514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8f6a690",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0faad2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.P[state][action][0]:  (1.0, 223, -1, False)\n"
     ]
    }
   ],
   "source": [
    "# env.P is double dictionary.\n",
    "# - The 1st key represents the state, from 0 to 499\n",
    "# - The 2nd key represens the action taken by the agent,\n",
    "#   from 0 to 5\n",
    "\n",
    "# example\n",
    "state = 123\n",
    "action = 0  # move south\n",
    "\n",
    "# env.P[state][action][0] is a list with 4 elements\n",
    "# (probability, next_state, reward, done)\n",
    "# \n",
    "#  - probability\n",
    "#    It is always 1 in this environment, which means\n",
    "#    there are no external/random factors that determine the\n",
    "#    next_state\n",
    "#    apart from the agent's action a.\n",
    "#\n",
    "#  - next_state: 223 in this case\n",
    "# \n",
    "#  - reward: -1 in this case\n",
    "#\n",
    "#  - done: boolean (True/False) indicates whether the\n",
    "#    episode has ended (i.e. the driver has dropped the\n",
    "#    passenger at the correct destination)\n",
    "print('env.P[state][action][0]: ', env.P[state][action][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552caf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to call reset() at least once before render() will work\n",
    "env.reset()\n",
    "\n",
    "# env.s = 123\n",
    "# env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ded2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.s = 223\n",
    "# env.render(mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aacea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449,\n",
       " -1,\n",
       " False,\n",
       " False,\n",
       " {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca87838",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3abe48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
